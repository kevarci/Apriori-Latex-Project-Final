{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Utilidades generales\n",
    "import time\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Algoritmos y matrices\n",
    "from scipy.sparse import csr_matrix\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento y limpieza de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar datases\n",
    "orders= pd.read_csv('order_products__train.csv')\n",
    "products = pd.read_csv('products.csv')\n",
    "\n",
    "# Limpieza de la base de datos\n",
    "products['product_name'] = products['product_name'].replace(\n",
    "    to_replace=\"[^a-zA-Z\\\\d\\\\s]+\",\n",
    "    value=\"~\",\n",
    "    regex=True\n",
    ").str.lower().str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bulgarian yogurt', 'organic 4~ milk fat whole milk cottage cheese', 'cucumber kirby', 'organic celery hearts', 'lightly smoked sardines in olive oil', 'organic hass avocado', 'bag of organic bananas', 'organic whole string cheese']\n",
      "['grated pecorino romano cheese', 'spring water', 'super greens salad', 'cage free extra large grade aa eggs', 'prosciutto~ americano', 'organic garnet sweet potato ~yam~', 'asparagus', 'organic half ~ half']\n",
      "['shelled pistachios', 'organic raw unfiltered apple cider vinegar', 'organic hot house tomato', 'organic baby arugula', 'green peas', 'bunched cilantro', 'flat parsley~ bunch', 'organic biologique limes', 'fresh dill']\n",
      "['organic raspberries', 'organic whole strawberries', 'organic blueberries', 'organic grape tomatoes', 'organic cucumber', 'roasted turkey', 'organic pomegranate kernels']\n",
      "['natural spring water', 'organic unsweetened almond milk', 'tomatoes~ crushed~ organic', 'organic sliced provalone cheese', 'organic chocolate almondmilk pudding', 'bag of organic bananas', 'whole milk greek blended vanilla bean yogurt', 'organic orange juice with calcium ~ vitamin d', 'guacamole', 'uncured applewood smoked bacon', 'organic extra virgin oil olive', 'organic raspberries', 'black beans', 'organic seasoned yukon select potatoes hashed browns', 'geranium liquid dish soap', 'organic ketchup', 'raspberry sorbet pops', '100~ organic unbleached all~purpose flour', 'lavender scent laundry detergent', 'organic italian parsley bunch', 'organic garlic', 'organic raw kombucha gingerade', 'organic 2~ buttermilk', 'organic cinnamon apple sauce', 'unsalted cultured butter', 'organic hothouse cucumbers', 'black beans no salt added', 'uncured genoa salami', 'sliced pepperoni', 'natural chicken ~ maple breakfast sausage patty', 'queso fresco', 'crackers~ oyster', 'organic free range chicken broth', 'organic yellow onion', 'organic zucchini', 'organic whole grassmilk milk', 'mild diced green chiles', 'pinto beans no salt added', 'organic lemonade', 'corn maize tortillas', 'organic corn starch', 'olive oil ~ aloe vera hand soap', 'plastic spoons', 'aluminum foil', 'plastic wrap', 'organic coconut milk', 'garbanzo beans', 'organic stringles mozzarella string cheese', 'baby swiss slices cheese']\n"
     ]
    }
   ],
   "source": [
    "# Mege y agrupación por No de  orden\n",
    "dfMerged = pd.merge(orders, products, on=\"product_id\", how=\"inner\")\n",
    "dfMerged = dfMerged.sort_values(\"order_id\").reset_index(drop=True)\n",
    "\n",
    "# Agrupar productos por transacción (cada pedido (order_id) se agrupa en una lista de productos)\n",
    "transactions = dfMerged.groupby('order_id')['product_name'].apply(list).tolist()\n",
    "\n",
    "for t in transactions[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transacciones exportadas a 'TransactionsInstacart.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Exportando las transacciones a CSV (formato basket) para su uso posterior en R.\n",
    "\n",
    "output_csv = \"TransactionsInstacart.csv\"\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    for trans in transactions:\n",
    "        f.write(\",\".join(trans) + \"\\n\")\n",
    "print(f\"Transacciones exportadas a '{output_csv}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de Martiz dispersa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz dispersa creada con forma: (131209, 39043)\n"
     ]
    }
   ],
   "source": [
    "# Se crea un mapeo de cada producto a un índice.\n",
    "item_mapping = {item: idx for idx, item in enumerate(sorted({item for transaction in transactions for item in transaction}))}\n",
    "\n",
    "# Recorremos cada transacción para construir las coordenadas (filas y columnas) en la matriz.\n",
    "rows, cols = [], []\n",
    "for row_idx, trans in enumerate(transactions):\n",
    "    for item in trans:\n",
    "        rows.append(row_idx)\n",
    "        cols.append(item_mapping[item])\n",
    "\n",
    "order_matrix_sparse = csr_matrix(([1] * len(rows), (rows, cols)), shape=(len(transactions), len(item_mapping)))\n",
    "print(f\"Matriz dispersa creada con forma: {order_matrix_sparse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la matriz dispersa a una lista de transacciones (lista de productos comprados)\n",
    "transactions_list = []\n",
    "for row in order_matrix_sparse:\n",
    "    non_zero_indices = row.nonzero()[1]  # Encuentra los índices no cero\n",
    "    transaction = [list(item_mapping.keys())[index] for index in non_zero_indices]\n",
    "    transactions_list.append(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación Manual del Algoritmo Apriori en Python (Procesamiento por Lotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_candidatos(itemsets_frecuentes_k, k, max_candidates=100000, support_dict=None):\n",
    "    \"\"\"\n",
    "    Genera candidatos (k+1)-itemsets a partir de itemsets frecuentes de tamaño k.\n",
    "    Aplica límites si hay demasiados itemsets y usa un índice por prefijo para mayor eficiencia.\n",
    "    \"\"\"\n",
    "    if len(itemsets_frecuentes_k) > 1000:\n",
    "        print(f\"Demasiados itemsets frecuentes ({len(itemsets_frecuentes_k)}). Limitando a los 1000 más frecuentes.\")\n",
    "        if support_dict:\n",
    "            itemsets_frecuentes_k = sorted(itemsets_frecuentes_k,\n",
    "                                           key=lambda x: support_dict.get(tuple(x), 0),\n",
    "                                           reverse=True)[:1000]\n",
    "        else:\n",
    "            itemsets_frecuentes_k = itemsets_frecuentes_k[:1000]\n",
    "\n",
    "    n = len(itemsets_frecuentes_k)\n",
    "    estimated_candidates = n * (n - 1) // 2\n",
    "    if estimated_candidates > max_candidates:\n",
    "        reduction_factor = max_candidates / estimated_candidates\n",
    "        limit = int(n * (reduction_factor ** 0.5))\n",
    "        print(f\"Estimación de candidatos ({estimated_candidates}) excede el máximo. Limitando a {limit} itemsets.\")\n",
    "        if support_dict:\n",
    "            itemsets_frecuentes_k = sorted(itemsets_frecuentes_k,\n",
    "                                           key=lambda x: support_dict.get(tuple(x), 0),\n",
    "                                           reverse=True)[:limit]\n",
    "        else:\n",
    "            itemsets_frecuentes_k = itemsets_frecuentes_k[:limit]\n",
    "        n = len(itemsets_frecuentes_k)\n",
    "\n",
    "    candidatos_set = set()\n",
    "    if k == 1:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                candidato = tuple(sorted([itemsets_frecuentes_k[i][0], itemsets_frecuentes_k[j][0]]))\n",
    "                candidatos_set.add(candidato)\n",
    "    else:\n",
    "        # Crear índice por prefijo (los primeros k-1 elementos)\n",
    "        prefix_index = {}\n",
    "        for idx, itemset in enumerate(itemsets_frecuentes_k):\n",
    "            prefix = tuple(itemset[:k-1])\n",
    "            prefix_index.setdefault(prefix, []).append(idx)\n",
    "        for prefix, indices in prefix_index.items():\n",
    "            if len(indices) > 1:\n",
    "                for i in range(len(indices)):\n",
    "                    for j in range(i+1, len(indices)):\n",
    "                        idx1, idx2 = indices[i], indices[j]\n",
    "                        candidato = list(prefix) + [itemsets_frecuentes_k[idx1][k-1],\n",
    "                                                    itemsets_frecuentes_k[idx2][k-1]]\n",
    "                        candidato.sort()\n",
    "                        candidatos_set.add(tuple(candidato))\n",
    "\n",
    "    candidatos_unicos = [list(x) for x in candidatos_set]\n",
    "    if len(candidatos_unicos) > max_candidates:\n",
    "        print(f\"Aún hay demasiados candidatos ({len(candidatos_unicos)}). Limitando a {max_candidates}.\")\n",
    "        candidatos_unicos = candidatos_unicos[:max_candidates]\n",
    "\n",
    "    return candidatos_unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización en lugar de usar sorted en cada iteración dentro de all , se preordenan los candidatos y se optimiza la comparación\n",
    "def poda_apriori(candidatos, itemsets_frecuentes_k, k):\n",
    "    \"\"\"\n",
    "    Poda candidatos eliminando aquellos cuyo subconjunto (de tamaño k) no es frecuente.\n",
    "    \"\"\"\n",
    "    itemsets_frecuentes_set = set(tuple(sorted(x)) for x in itemsets_frecuentes_k)  # Preordena los itemsets frecuentes\n",
    "    candidatos_podados = [candidato for candidato in candidatos if all(tuple(sorted(candidato[:i] + candidato[i+1:])) in itemsets_frecuentes_set for i in range(len(candidato)))]\n",
    "    return candidatos_podados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_lotes(transactions_list, min_support, batch_size=50000):\n",
    "    \"\"\"\n",
    "    Implementación manual del algoritmo Apriori usando procesamiento por lotes.\n",
    "    Calcula 1-itemsets y itemsets de mayor tamaño, usando un diccionario para los soportes.\n",
    "    \"\"\"\n",
    "    n_total = len(transactions_list)\n",
    "    print(f\"Procesando {n_total} transacciones con soporte mínimo {min_support}\")\n",
    "\n",
    "    # Fase 1: Contar 1-itemsets en lotes\n",
    "    item_counts = {}\n",
    "    n_batches = (n_total + batch_size - 1) // batch_size\n",
    "    print(f\"Fase 1: Generando 1-itemsets frecuentes en {n_batches} lotes.\")\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_total)\n",
    "        print(f\"  Procesando lote {i+1}/{n_batches} (transacciones {start_idx} - {end_idx})\")\n",
    "        batch_transactions = transactions_list[start_idx:end_idx]\n",
    "        for transaction in batch_transactions:\n",
    "            for item in transaction:\n",
    "                item_counts[item] = item_counts.get(item, 0) + 1\n",
    "\n",
    "    frequent_1_itemsets = []\n",
    "    support_dict = {}\n",
    "    for item, count in item_counts.items():\n",
    "        support = count / n_total\n",
    "        if support >= min_support:\n",
    "            frequent_1_itemsets.append([item])\n",
    "            support_dict[tuple([item])] = support\n",
    "    print(f\"Se encontraron {len(frequent_1_itemsets)} 1-itemsets frecuentes.\")\n",
    "\n",
    "    all_frequent_itemsets = {1: frequent_1_itemsets}\n",
    "    k = 1\n",
    "    while all_frequent_itemsets.get(k, []):\n",
    "        print(f\"Fase {k+1}: Generando {k+1}-itemsets frecuentes.\")\n",
    "        if len(all_frequent_itemsets[k]) > 1000:\n",
    "            print(f\"  Demasiados itemsets frecuentes ({len(all_frequent_itemsets[k])}). Limitando a 1000 con mayor soporte.\")\n",
    "            sorted_itemsets = sorted(all_frequent_itemsets[k],\n",
    "                                     key=lambda x: support_dict.get(tuple(x), 0),\n",
    "                                     reverse=True)[:1000]\n",
    "            all_frequent_itemsets[k] = sorted_itemsets\n",
    "\n",
    "        candidatos = generar_candidatos(all_frequent_itemsets[k], k, support_dict=support_dict)\n",
    "        if len(candidatos) > 100000:\n",
    "            print(f\"  Demasiados candidatos ({len(candidatos)}). Aumentando temporalmente el umbral de soporte.\")\n",
    "            temp_min_support = min_support * 2\n",
    "            filtered_itemsets = [itemset for itemset in all_frequent_itemsets[k] if support_dict.get(tuple(itemset), 0) >= temp_min_support]\n",
    "            candidatos = generar_candidatos(filtered_itemsets, k, support_dict=support_dict)\n",
    "\n",
    "        candidatos_podados = poda_apriori(candidatos, all_frequent_itemsets[k], k)\n",
    "        print(f\"  Generados {len(candidatos)} candidatos, {len(candidatos_podados)} después de poda.\")\n",
    "        if not candidatos_podados:\n",
    "            break\n",
    "\n",
    "        # Contar soporte de candidatos en lotes\n",
    "        candidato_counts = {tuple(c): 0 for c in candidatos_podados}\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_total)\n",
    "            print(f\"  Contando candidatos en lote {i+1}/{n_batches} (transacciones {start_idx} - {end_idx})\")\n",
    "            batch_transactions = transactions_list[start_idx:end_idx]\n",
    "            for transaction in batch_transactions:\n",
    "                transaction_set = set(transaction)\n",
    "                for candidato in candidatos_podados:\n",
    "                    if all(item in transaction_set for item in candidato):\n",
    "                        candidato_counts[tuple(candidato)] += 1\n",
    "\n",
    "        frequent_itemsets_k_plus_1 = []\n",
    "        for candidato, count in candidato_counts.items():\n",
    "            support = count / n_total\n",
    "            if support >= min_support:\n",
    "                frequent_itemsets_k_plus_1.append(list(candidato))\n",
    "                support_dict[tuple(sorted(candidato))] = support\n",
    "\n",
    "        print(f\"  Se encontraron {len(frequent_itemsets_k_plus_1)} {k+1}-itemsets frecuentes.\")\n",
    "        if frequent_itemsets_k_plus_1:\n",
    "            all_frequent_itemsets[k+1] = frequent_itemsets_k_plus_1\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "    return all_frequent_itemsets, support_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_reglas(itemsets_frecuentes, support_dict, min_confidence=0.3):\n",
    "    \"\"\"\n",
    "    Genera reglas de asociación a partir de itemsets frecuentes.\n",
    "    Para cada itemset (de tamaño >= 2), genera todas las posibles divisiones:\n",
    "    antecedente y consecuente, calcula métricas (soporte, confianza, lift) y\n",
    "    retorna un DataFrame con las reglas ordenadas por lift.\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for k in range(2, len(itemsets_frecuentes) + 1):\n",
    "        if k not in itemsets_frecuentes:\n",
    "            continue\n",
    "        for itemset in itemsets_frecuentes[k]:\n",
    "            itemset_support = support_dict.get(tuple(sorted(itemset)), 0)\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedente_items in combinations(itemset, i):\n",
    "                    antecedente = list(antecedente_items)\n",
    "                    consecuente = [item for item in itemset if item not in antecedente]\n",
    "                    if not antecedente or not consecuente:\n",
    "                        continue\n",
    "                    antecedente_support = support_dict.get(tuple(sorted(antecedente)), 0)\n",
    "                    if antecedente_support == 0:\n",
    "                        continue\n",
    "                    confidence = itemset_support / antecedente_support\n",
    "                    if confidence >= min_confidence:\n",
    "                        consecuente_support = support_dict.get(tuple(sorted(consecuente)), 0)\n",
    "                        if consecuente_support == 0:\n",
    "                            continue\n",
    "                        lift = confidence / consecuente_support\n",
    "                        rules.append({\n",
    "                            'antecedent': antecedente,\n",
    "                            'consequent': consecuente,\n",
    "                            'support': itemset_support,\n",
    "                            'confidence': confidence,\n",
    "                            'lift': lift\n",
    "                        })\n",
    "    if rules:\n",
    "        rulesDf = pd.DataFrame(rules)\n",
    "        rulesDf = rulesDf.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        return rulesDf\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['antecedent', 'consequent', 'support', 'confidence', 'lift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Apriori Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 131209 transacciones con soporte mínimo 0.01\n",
      "Fase 1: Generando 1-itemsets frecuentes en 3 lotes.\n",
      "  Procesando lote 1/3 (transacciones 0 - 50000)\n",
      "  Procesando lote 2/3 (transacciones 50000 - 100000)\n",
      "  Procesando lote 3/3 (transacciones 100000 - 131209)\n",
      "Se encontraron 104 1-itemsets frecuentes.\n",
      "Fase 2: Generando 2-itemsets frecuentes.\n",
      "  Generados 5356 candidatos, 5356 después de poda.\n",
      "  Contando candidatos en lote 1/3 (transacciones 0 - 50000)\n",
      "  Contando candidatos en lote 2/3 (transacciones 50000 - 100000)\n",
      "  Contando candidatos en lote 3/3 (transacciones 100000 - 131209)\n",
      "  Se encontraron 16 2-itemsets frecuentes.\n",
      "Fase 3: Generando 3-itemsets frecuentes.\n",
      "  Generados 22 candidatos, 7 después de poda.\n",
      "  Contando candidatos en lote 1/3 (transacciones 0 - 50000)\n",
      "  Contando candidatos en lote 2/3 (transacciones 50000 - 100000)\n",
      "  Contando candidatos en lote 3/3 (transacciones 100000 - 131209)\n",
      "  Se encontraron 0 3-itemsets frecuentes.\n",
      "\n",
      "Reglas generadas manualmente (primeras 5):\n",
      "               antecedent                consequent   support  confidence  \\\n",
      "0   [organic raspberries]    [organic strawberries]  0.012728    0.301118   \n",
      "1  [organic hass avocado]  [bag of organic bananas]  0.018444    0.331825   \n",
      "2   [organic raspberries]  [bag of organic bananas]  0.013566    0.320952   \n",
      "\n",
      "      lift  \n",
      "0  3.62671  \n",
      "1  2.81256  \n",
      "2  2.72040  \n",
      "\n",
      "Tiempo total del Apriori manual (itemsets + reglas): 425.09 segundos\n",
      "\n",
      "Reglas generadas manualmente (primeras 5):\n",
      "               antecedent                consequent   support  confidence  \\\n",
      "0   [organic raspberries]    [organic strawberries]  0.012728    0.301118   \n",
      "1  [organic hass avocado]  [bag of organic bananas]  0.018444    0.331825   \n",
      "2   [organic raspberries]  [bag of organic bananas]  0.013566    0.320952   \n",
      "\n",
      "      lift  \n",
      "0  3.62671  \n",
      "1  2.81256  \n",
      "2  2.72040  \n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el algoritmo Apriori manual\n",
    "min_support = 0.01\n",
    "min_confidence = 0.3\n",
    "\n",
    "# Aquí usamos la lista de transacciones obtenida antes\n",
    "transactions_list = transactions\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_frequent_itemsets, support_dict = apriori_lotes(transactions_list, min_support)\n",
    "\n",
    "# Generar reglas a partir de los itemsets frecuentes\n",
    "rules_manual = generar_reglas(all_frequent_itemsets, support_dict, min_confidence=0.3)\n",
    "print(\"\\nReglas generadas manualmente (primeras 5):\")\n",
    "print(rules_manual.head())\n",
    "\n",
    "\n",
    "manual_time = time.time() - start_time\n",
    "print(f\"\\nTiempo total del Apriori manual (itemsets + reglas): {manual_time:.2f} segundos\")\n",
    "\n",
    "\n",
    "print(\"\\nReglas generadas manualmente (primeras 5):\")\n",
    "print(rules_manual.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Apriori con mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo total del Apriori con mlxtend (itemsets + reglas): 83.29 segundos\n",
      "\n",
      "Itemsets frecuentes generados con mlxtend:\n",
      "    support                    itemsets\n",
      "0  0.017514    (100~ whole wheat bread)\n",
      "1  0.011737       (2~ reduced fat milk)\n",
      "2  0.017163  (apple honeycrisp organic)\n",
      "3  0.029480                 (asparagus)\n",
      "4  0.117980    (bag of organic bananas)\n",
      "\n",
      "Reglas generadas con mlxtend (primeras 5):\n",
      "                antecedents               consequents  antecedent support  \\\n",
      "0  (bag of organic bananas)    (organic baby spinach)            0.117980   \n",
      "1    (organic baby spinach)  (bag of organic bananas)            0.074568   \n",
      "2  (bag of organic bananas)    (organic hass avocado)            0.117980   \n",
      "3    (organic hass avocado)  (bag of organic bananas)            0.055583   \n",
      "4  (bag of organic bananas)     (organic raspberries)            0.117980   \n",
      "\n",
      "   consequent support   support  confidence      lift  representativity  \\\n",
      "0            0.074568  0.017042    0.144444  1.937082               1.0   \n",
      "1            0.117980  0.017042    0.228536  1.937082               1.0   \n",
      "2            0.055583  0.018444    0.156331  2.812560               1.0   \n",
      "3            0.117980  0.018444    0.331825  2.812560               1.0   \n",
      "4            0.042268  0.013566    0.114987  2.720400               1.0   \n",
      "\n",
      "   leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0  0.008244    1.081674       0.548468  0.097099   0.075507    0.186490  \n",
      "1  0.008244    1.143308       0.522739  0.097099   0.125345    0.186490  \n",
      "2  0.011886    1.119416       0.730654  0.118901   0.106677    0.244078  \n",
      "3  0.011886    1.320044       0.682381  0.118901   0.242449    0.244078  \n",
      "4  0.008579    1.082167       0.716998  0.092487   0.075928    0.217970  \n"
     ]
    }
   ],
   "source": [
    "# Convertir la lista de transacciones en formato binarizado para la librería\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Cronometrar todo el flujo de Apriori con mlxtend\n",
    "start_time_mlxtend = time.time()\n",
    "\n",
    "# Generar itemsets frecuentes\n",
    "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Generar reglas de asociación\n",
    "rules_mlxtend = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Detener el cronómetro\n",
    "mlxtend_time = time.time() - start_time_mlxtend\n",
    "print(f\"\\nTiempo total del Apriori con mlxtend (itemsets + reglas): {mlxtend_time:.2f} segundos\")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nItemsets frecuentes generados con mlxtend:\")\n",
    "print(frequent_itemsets.head())\n",
    "print(\"\\nReglas generadas con mlxtend (primeras 5):\")\n",
    "print(rules_mlxtend.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno Híbrido con R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rpy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# En este oasi ejecutal el pip install rpy2 en la terminal, luego ejecuta el codigo. Esto se debe aque si lo trabjamos en un IDE local este paso se procede en la terminal\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrpy2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrobjects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrobjects\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrpy2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrobjects\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m importr\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rpy2'"
     ]
    }
   ],
   "source": [
    "# En este oasi ejecutal el pip install rpy2 en la terminal, luego ejecuta el codigo. Esto se debe aque si lo trabjamos en un IDE local este paso se procede en la terminal\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la extensión de R para poder ejecutar código R en celdas de Jupyter\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "# Esta celda ejecutará código R (todo lo que esté después de esta línea será interpretado como código R)\n",
    "%%R\n",
    "install.packages(\"arules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o r_execution_time,rules_df\n",
    "# Cargar el paquete arules\n",
    "if (!require(\"arules\")) {\n",
    "    install.packages(\"arules\", repos=\"http://cran.us.r-project.org\")\n",
    "    library(arules)\n",
    "}\n",
    "\n",
    "# Leer las transacciones desde el archivo exportado por Python\n",
    "baskets <- read.transactions(\"TransactionsInstacart.csv\", sep = \",\", format = \"basket\")\n",
    "\n",
    "# Visualizar la frecuencia de ítems\n",
    "itemFrequencyPlot(baskets, support = 0.055, main = \"Frecuencia de ítems (>= 5.5% soporte)\")\n",
    "start_r_time <- Sys.time()\n",
    "\n",
    "# Generar reglas de asociación con Apriori\n",
    "rules_r <- apriori(baskets, parameter = list(support = 0.001, confidence = 0.3, minlen = 2, maxlen = 15))\n",
    "rules_r <- sort(rules_r, by = \"lift\")\n",
    "\n",
    "# Detener el cronómetro\n",
    "end_r_time <- Sys.time()\n",
    "r_execution_time <- end_r_time - start_r_time\n",
    "print(paste(\"Tiempo total de procesamiento en R:\", r_execution_time, \"segundos\"))\n",
    "\n",
    "# Inspeccionar las primeras reglas\n",
    "inspect(rules_r[1:5])\n",
    "\n",
    "# Convertir las reglas en un data.frame y añadir columnas\n",
    "rules_df <- as(rules_r, \"data.frame\")\n",
    "rules_df$antecedent <- labels(lhs(rules_r))  # Extrae el lado izquierdo (lhs)\n",
    "rules_df$consequent <- labels(rhs(rules_r))  # Extrae el lado derecho (rhs)\n",
    "\n",
    "# Exportar a un archivo CSV\n",
    "write.csv(rules_df, file = \"rulesdatabase_r.csv\", row.names = FALSE, quote = TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #se utiliza aquí para verificar si un archivo (rulesdatabase_r.csv) existe en el sistema de archivos.\n",
    "\n",
    "\n",
    "# Verifica si el archivo generado por R está disponible\n",
    "if os.path.exists(\"rulesdatabase_r.csv\"):\n",
    "    # Cargar las reglas generadas por R/arules\n",
    "    rules_r_df = pd.read_csv(\"rulesdatabase_r.csv\")\n",
    "    rules_r_df.sort_values(by=\"lift\", ascending=False, inplace=True)\n",
    "    rules_r_df.reset_index(drop=True, inplace=True)\n",
    "    print(\"\\nReglas generadas por R (primeras 5):\")\n",
    "    print(rules_r_df.head())\n",
    "else:\n",
    "    print(\"El archivo 'rulesdatabase_r.csv' no se encontró. Asegúrate de haber generado las reglas en R.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las reglas generadas desde R\n",
    "rules_r = pd.read_csv(\"rulesdatabase_r.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de las reglas generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_reglas(rules_py, rules_ml, rules_r):\n",
    "    \"\"\"\n",
    "    Compara reglas generadas manualmente, con mlxtend, y en R.\n",
    "    \"\"\"\n",
    "    # Normalizar `rules_mlxtend`: convertir `antecedents` y `consequents` de conjuntos a listas ordenadas\n",
    "    rules_ml['antecedents'] = rules_ml['antecedents'].apply(lambda x: sorted(list(x)) if isinstance(x, set) else x)\n",
    "    rules_ml['consequents'] = rules_ml['consequents'].apply(lambda x: sorted(list(x)) if isinstance(x, set) else x)\n",
    "\n",
    "    # `rules_r` ya tiene las columnas correctas: `antecedent` y `consequent`\n",
    "    matches = []\n",
    "    differences = []\n",
    "\n",
    "    for _, rule_py in rules_py.iterrows():\n",
    "        # Buscar regla equivalente en `mlxtend`\n",
    "        matched_ml = rules_ml[\n",
    "            (rules_ml['antecedents'] == sorted(rule_py['antecedent'])) &\n",
    "            (rules_ml['consequents'] == sorted(rule_py['consequent']))\n",
    "        ]\n",
    "\n",
    "        # Buscar regla equivalente en `R`\n",
    "        matched_r = rules_r[\n",
    "            (rules_r['antecedent'] == sorted(rule_py['antecedent'])) &\n",
    "            (rules_r['consequent'] == sorted(rule_py['consequent']))\n",
    "        ]\n",
    "\n",
    "        if not matched_ml.empty and not matched_r.empty:\n",
    "            matched_ml_row = matched_ml.iloc[0]\n",
    "            matched_r_row = matched_r.iloc[0]\n",
    "            matches.append({\n",
    "                'antecedent': rule_py['antecedent'],\n",
    "                'consequent': rule_py['consequent'],\n",
    "                'support_manual': rule_py['support'],\n",
    "                'support_ml': matched_ml_row['support'],\n",
    "                'support_r': matched_r_row['support'],\n",
    "                'confidence_manual': rule_py['confidence'],\n",
    "                'confidence_ml': matched_ml_row['confidence'],\n",
    "                'confidence_r': matched_r_row['confidence'],\n",
    "                'lift_manual': rule_py['lift'],\n",
    "                'lift_ml': matched_ml_row['lift'],\n",
    "                'lift_r': matched_r_row['lift']\n",
    "            })\n",
    "        else:\n",
    "           # Si la regla no está en una o más implementaciones\n",
    "            differences.append({\n",
    "                'antecedent': rule_py['antecedent'],\n",
    "                'consequent': rule_py['consequent'],\n",
    "                'reason': 'No encontrada en mlxtend o R'\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(matches), pd.DataFrame(differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recopilar datos\n",
    "\n",
    "\n",
    "resultados = [\n",
    "    {\n",
    "        \"Reglas generadas\": len(rules_manual),  # Número de reglas generadas manualmente\n",
    "        \"Soporte mínimo\": \"1%\",\n",
    "        \"Confianza promedio\": round(rules_manual[\"confidence\"].mean(), 2),  # Promedio de confianza en reglas manuales\n",
    "        \"Lift promedio\": round(rules_manual[\"lift\"].mean(), 2),  # Promedio de lift en reglas manuales\n",
    "        \"Tiempo (Python manual)\": round(manual_time, 2),  # Tiempo total calculado para la implementación manual\n",
    "        \"Tiempo (`mlxtend`)\": round(mlxtend_time, 2),    # Tiempo total calculado con mlxtend\n",
    "        \"Tiempo (R)\": round(float(r_execution_time[0]), 2),  # Tiempo calculado en R (convertido a segundos)\n",
    "\n",
    "\n",
    "\n",
    "    },\n",
    "]\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Visualizar la tabla\n",
    "print(\"\\nTabla comparativa de tiempos y resultados:\")\n",
    "from IPython.display import display\n",
    "display(df_resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos para el gráfico\n",
    "soportes = df_resultados[\"Soporte mínimo\"]\n",
    "tiempos_python = df_resultados[\"Tiempo (Python manual)\"]\n",
    "tiempos_mlxtend = df_resultados[\"Tiempo (`mlxtend`)\"]\n",
    "tiempos_r = df_resultados[\"Tiempo (R)\"]\n",
    "confianza_prom = df_resultados[\"Confianza promedio\"]\n",
    "lift_prom = df_resultados[\"Lift promedio\"]\n",
    "reglas_generadas = df_resultados[\"Reglas generadas\"]\n",
    "\n",
    "# Gráfico 1: Comparación de tiempos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(soportes, tiempos_python, label=\"Python manual\", marker=\"o\")\n",
    "plt.plot(soportes, tiempos_mlxtend, label=\"`mlxtend`\", marker=\"o\")\n",
    "plt.plot(soportes, tiempos_r, label=\"R\", marker=\"o\")\n",
    "plt.title(\"Comparación de Tiempos de Ejecución\", fontsize=14)\n",
    "plt.xlabel(\"Soporte mínimo\", fontsize=12)\n",
    "plt.ylabel(\"Tiempo (segundos)\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Recomendaciones Basadas en una Transacción Existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules_df.columns)\n",
    "print(rules_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir una cadena del antecedente a lista (si es necesario)\n",
    "def convertir_a_lista(cadena):\n",
    "    cadena = cadena.strip(\"{}\")\n",
    "    if not cadena:\n",
    "        return []\n",
    "    return [item.strip() for item in cadena.split(',')]\n",
    "\n",
    "# Función para evaluar coincidencia parcial\n",
    "def coincide_parcial(antecedente, transaccion, umbral=0.5):\n",
    "    # Retorna True si al menos \"umbral\" (por defecto 50%) de los ítems del antecedente están en la transacción\n",
    "    if not antecedente:\n",
    "        return False\n",
    "    interseccion = set(antecedente).intersection(set(transaccion))\n",
    "    return len(interseccion) / len(antecedente) >= umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar una transacción existente por indice\n",
    "indice_transaccion = 301  # Modifica este valor para probar con distintos índices\n",
    "transaccion_existente = transactions[indice_transaccion]\n",
    "print(f\"Usando la transacción existente en el índice {indice_transaccion}: {transaccion_existente}\")\n",
    "\n",
    "# Filtrar reglas (obtenidas desde R y almacenadas en 'rules_df') usando el criterio de coincidencia parcial\n",
    "productos_recomendados = []\n",
    "for _, regla in rules_df.iterrows():\n",
    "    # Convertir el antecedente (si viene como cadena) a una lista de ítems\n",
    "    antecedente = convertir_a_lista(regla['antecedent'])\n",
    "    # Se evalúa la coincidencia parcial: al menos el 50% de los ítems del antecedente deben estar en la transacción\n",
    "    if coincide_parcial(antecedente, transaccion_existente, umbral=0.5):\n",
    "        productos_recomendados.append({\n",
    "            'producto': regla['consequent'],\n",
    "            'lift': regla['lift'],\n",
    "            'confidence': regla['confidence'],\n",
    "            'support': regla['support']\n",
    "        })\n",
    "\n",
    "# Convertir la lista de recomendaciones a un DataFrame definiendo las columnas esperadas\n",
    "import pandas as pd\n",
    "columnas = ['producto', 'lift', 'confidence', 'support']\n",
    "recomendaciones_df = pd.DataFrame(productos_recomendados, columns=columnas)\n",
    "\n",
    "# Verificar si se encontraron recomendaciones\n",
    "if recomendaciones_df.empty:\n",
    "    print(\"No se encontraron recomendaciones para esta transacción.\")\n",
    "else:\n",
    "    # Eliminar duplicados y ordenar las recomendaciones por 'lift'\n",
    "    recomendaciones_df = recomendaciones_df.drop_duplicates(subset='producto').sort_values(by='lift', ascending=False)\n",
    "    print(\"Productos recomendados basados en la transacción existente:\")\n",
    "    print(recomendaciones_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización interactiva con Plotly\n",
    "import plotly.express as px\n",
    "fig = px.scatter(\n",
    "    recomendaciones_df,\n",
    "    x='support',\n",
    "    y='confidence',\n",
    "    size='lift',\n",
    "    color='lift',\n",
    "    hover_data=['producto'],\n",
    "    title=f\"Productos Recomendados Basados en Transacción Existente (Índice {indice_transaccion})\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Soporte\",\n",
    "    yaxis_title=\"Confianza\",\n",
    "    legend_title=\"Lift\",\n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Grafo interactivo con la transacción existente\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Crear grafo para la transacción existente\n",
    "G = nx.Graph()\n",
    "\n",
    "# Añadir nodos para la transacción inicial\n",
    "for producto in transaccion_existente:\n",
    "    G.add_node(producto, type='comprado', color='blue', size=30)\n",
    "\n",
    "# Añadir nodos para las recomendaciones\n",
    "for _, row in recomendaciones_df.iterrows():\n",
    "    G.add_node(row['producto'], type='recomendado', color='orange', size=row['lift']*10)\n",
    "    for producto in transaccion_existente:\n",
    "        G.add_edge(producto, row['producto'], weight=row['lift'])\n",
    "\n",
    "# Extraer posiciones para los nodos\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Nodos del grafo\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_size = []\n",
    "node_color = []\n",
    "node_text = []\n",
    "for node in G.nodes(data=True):\n",
    "    node_x.append(pos[node[0]][0])\n",
    "    node_y.append(pos[node[0]][1])\n",
    "    node_size.append(node[1]['size'])\n",
    "    node_color.append(node[1]['color'])\n",
    "    node_text.append(node[0])\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode='markers',\n",
    "    marker=dict(size=node_size, color=node_color, line=dict(width=1)),\n",
    "    text=node_text,\n",
    "    hoverinfo='text'\n",
    ")\n",
    "\n",
    "# Conexiones del grafo\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges(data=True):\n",
    "    edge_x.extend([pos[edge[0]][0], pos[edge[1]][0], None])\n",
    "    edge_y.extend([pos[edge[0]][1], pos[edge[1]][1], None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x,\n",
    "    y=edge_y,\n",
    "    line=dict(width=0.5, color='gray'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines'\n",
    ")\n",
    "\n",
    "# Visualización interactiva del grafo\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                layout=go.Layout(\n",
    "                    title=f\"Red Interactiva de Productos Comprados y Recomendados (Transacción Existente, Índice {indice_transaccion})\",\n",
    "                    showlegend=False,\n",
    "                    xaxis=dict(showgrid=False, zeroline=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False)\n",
    "                ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativas de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para generar la documentacion de complejidad\n",
    "def generar_dataset_sintetico(n_transacciones, n_items, longitud_media):\n",
    "\n",
    "    items = [f'item_{i}' for i in range(n_items)]\n",
    "    transactions = []\n",
    "\n",
    "    for _ in range(n_transacciones):\n",
    "        # Determinar longitud de esta transacción\n",
    "        longitud = max(1, int(np.random.normal(longitud_media, longitud_media/4)))\n",
    "        longitud = min(longitud, n_items)  # No puede ser mayor que el número total de items\n",
    "\n",
    "        # Generar transacción\n",
    "        transaction = random.sample(items, longitud)\n",
    "        transactions.append(transaction)\n",
    "\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_items_individuales(transactions):\n",
    "\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if item not in item_counts:\n",
    "                item_counts[item] = 0\n",
    "            item_counts[item] += 1\n",
    "    return item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion es para generar la documentacion de complejidad\n",
    "def generar_dataset_sintetico(n_transacciones, n_items, longitud_media):\n",
    "\n",
    "    items = [f'item_{i}' for i in range(n_items)]\n",
    "    transactions = []\n",
    "\n",
    "    for _ in range(n_transacciones):\n",
    "        # Determinar longitud de esta transacción\n",
    "        longitud = max(1, int(np.random.normal(longitud_media, longitud_media/4)))\n",
    "        longitud = min(longitud, n_items)  # No puede ser mayor que el número total de items\n",
    "\n",
    "        # Generar transacción\n",
    "        transaction = random.sample(items, longitud)\n",
    "        transactions.append(transaction)\n",
    "\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_original(transactions, min_support=0.5, max_length=None):\n",
    "\n",
    "    # Convertimos transacciones a conjuntos si no lo son ya\n",
    "    transactions = [set(transaction) for transaction in transactions]\n",
    "    n_transactions = len(transactions)\n",
    "\n",
    "\n",
    "    if isinstance(min_support, list) or isinstance(min_support, tuple):\n",
    "        min_support = float(min_support[0]) if min_support else 0.5\n",
    "\n",
    "    # Calcular el conteo mínimo como un número entero\n",
    "    min_support_count = int(min_support * n_transactions)\n",
    "\n",
    "    # Imprimir para depuración\n",
    "    # print(f\"DEBUG - min_support: {min_support} (tipo: {type(min_support)})\")\n",
    "    # print(f\"DEBUG - min_support_count: {min_support_count} (tipo: {type(min_support_count)})\")\n",
    "\n",
    "    # Paso 1: Generar 1-itemsets frecuentes\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if frozenset([item]) not in item_counts:\n",
    "                item_counts[frozenset([item])] = 0\n",
    "            item_counts[frozenset([item])] += 1\n",
    "\n",
    "    # Filtrar por soporte mínimo - usando un bucle explícito para evitar problemas\n",
    "    L1 = {}\n",
    "    for itemset, count in item_counts.items():\n",
    "        # Verificar que count sea un número\n",
    "        if isinstance(count, (int, float)) and isinstance(min_support_count, (int, float)):\n",
    "            if count >= min_support_count:\n",
    "                L1[itemset] = count\n",
    "\n",
    "    # Almacenar todos los itemsets frecuentes\n",
    "    all_frequent_itemsets = dict(L1)\n",
    "    current_L = L1\n",
    "    k = 2\n",
    "\n",
    "    # Iteración principal\n",
    "    while current_L and (max_length is None or k <= max_length):\n",
    "        # Paso 2: Generar candidatos de tamaño k\n",
    "        Ck = {}\n",
    "        for i in current_L:\n",
    "            for j in current_L:\n",
    "                # Unir itemsets que comparten k-2 elementos\n",
    "                union = i.union(j)\n",
    "                if len(union) == k:\n",
    "                    # Verificar si todos los subconjuntos son frecuentes\n",
    "                    all_subsets_frequent = True\n",
    "                    for item in union:\n",
    "                        subset = union - frozenset([item])\n",
    "                        if subset not in current_L:\n",
    "                            all_subsets_frequent = False\n",
    "                            break\n",
    "\n",
    "                    if all_subsets_frequent:\n",
    "                        Ck[union] = 0\n",
    "\n",
    "        # Paso 3: Contar soporte para candidatos\n",
    "        for transaction in transactions:\n",
    "            for candidate in Ck:\n",
    "                if candidate.issubset(transaction):\n",
    "                    Ck[candidate] += 1\n",
    "\n",
    "        # Filtrar por soporte mínimo\n",
    "        current_L = {itemset: count for itemset, count in Ck.items()\n",
    "                    if count >= min_support_count}\n",
    "\n",
    "        # Actualizar resultados\n",
    "        all_frequent_itemsets.update(current_L)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_rendimiento(datasets, min_supports, implementaciones, parametros=None):\n",
    "    if parametros is None:\n",
    "        # Por defecto, todas las implementaciones usan 'min_support'\n",
    "        parametros = {nombre: 'min_support' for nombre in implementaciones}\n",
    "\n",
    "    resultados = defaultdict(list)\n",
    "\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"\\nEvaluando dataset: {dataset_name}\")\n",
    "\n",
    "        for min_support in min_supports:\n",
    "            print(f\"\\n  Soporte mínimo: {min_support}\")\n",
    "\n",
    "            for nombre_impl, funcion_impl in implementaciones.items():\n",
    "                print(f\"    Ejecutando {nombre_impl}...\", end=\" \")\n",
    "\n",
    "                # Medir tiempo de ejecución\n",
    "                inicio = time.time()\n",
    "\n",
    "                # Usar el nombre de parámetro correcto para esta implementación\n",
    "                param_nombre = parametros.get(nombre_impl, 'min_support')\n",
    "                kwargs = {param_nombre: min_support}\n",
    "\n",
    "                frequent_itemsets = funcion_impl(dataset, **kwargs)\n",
    "                fin = time.time()\n",
    "\n",
    "                tiempo_ejecucion = fin - inicio\n",
    "                print(f\"Tiempo: {tiempo_ejecucion:.4f}s, Itemsets encontrados: {len(frequent_itemsets)}\")\n",
    "\n",
    "                # Guardar resultados\n",
    "                resultados[nombre_impl].append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'min_support': min_support,\n",
    "                    'tiempo': tiempo_ejecucion,\n",
    "                    'n_itemsets': len(frequent_itemsets)\n",
    "                })\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_resultados_simple(resultados, datasets, min_supports):\n",
    "\n",
    "    # Verificar si hay resultados\n",
    "    if not resultados:\n",
    "        print(\"No hay resultados para visualizar.\")\n",
    "        return\n",
    "\n",
    "    # Mostrar resultados en formato de tabla\n",
    "    print(\"\\n===== RESULTADOS DE LA COMPARACIÓN =====\")\n",
    "\n",
    "    # Para cada dataset\n",
    "    for dataset_name in datasets.keys():\n",
    "        print(f\"\\n\\nDATASET: {dataset_name}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Crear encabezado\n",
    "        header = \"| Soporte | \"\n",
    "        for impl_name in resultados.keys():\n",
    "            header += f\"{impl_name} (tiempo) | {impl_name} (itemsets) | \"\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "        # Mostrar resultados para cada soporte\n",
    "        for min_support in min_supports:\n",
    "            row = f\"| {min_support:.3f} | \"\n",
    "\n",
    "            for impl_name in resultados.keys():\n",
    "                # Buscar el resultado para este dataset, implementación y soporte\n",
    "                result = None\n",
    "                for r in resultados[impl_name]:\n",
    "                    if r['dataset'] == dataset_name and r['min_support'] == min_support:\n",
    "                        result = r\n",
    "                        break\n",
    "\n",
    "                if result:\n",
    "                    row += f\"{result['tiempo']:.4f}s | {result['n_itemsets']} | \"\n",
    "                else:\n",
    "                    row += \"N/A | N/A | \"\n",
    "\n",
    "            print(row)\n",
    "\n",
    "    # Crear gráfico de barras simple\n",
    "    print(\"\\n\\nCreando gráfico de barras...\")\n",
    "\n",
    "    # Configurar el gráfico\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Para cada dataset, crear un gráfico\n",
    "    for i, dataset_name in enumerate(datasets.keys()):\n",
    "        plt.subplot(len(datasets), 1, i+1)\n",
    "\n",
    "        # Preparar datos\n",
    "        bar_positions = np.arange(len(min_supports))\n",
    "        bar_width = 0.35\n",
    "\n",
    "        # Para cada implementación\n",
    "        for j, impl_name in enumerate(resultados.keys()):\n",
    "            # Extraer tiempos\n",
    "            tiempos = []\n",
    "            for min_s in min_supports:\n",
    "                tiempo = 0\n",
    "                for r in resultados[impl_name]:\n",
    "                    if r['dataset'] == dataset_name and r['min_support'] == min_s:\n",
    "                        tiempo = r['tiempo']\n",
    "                        break\n",
    "                tiempos.append(tiempo)\n",
    "\n",
    "            # Graficar barras\n",
    "            plt.bar(bar_positions + j*bar_width/len(resultados), tiempos,\n",
    "                   width=bar_width/len(resultados), label=impl_name)\n",
    "\n",
    "        # Configurar gráfico\n",
    "        plt.title(f'Comparación - {dataset_name}')\n",
    "        plt.xlabel('Soporte mínimo')\n",
    "        plt.ylabel('Tiempo (segundos)')\n",
    "        plt.xticks(bar_positions, [str(s) for s in min_supports])\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar y mostrar\n",
    "    ruta_guardado = r'c:\\Users\\stick\\Desktop\\4Geeks Academy Projects\\Apriori-Latex-Project-Final\\Apriori-Latex-Project-Final\\comparacion_rendimiento.png'\n",
    "    try:\n",
    "        plt.savefig(ruta_guardado)\n",
    "        print(f\"Gráfico guardado en: {ruta_guardado}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar el gráfico: {str(e)}\")\n",
    "\n",
    "    # Mostrar el gráfico\n",
    "    try:\n",
    "        plt.show()\n",
    "        print(\"Gráfico mostrado correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al mostrar el gráfico: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_comparacion():\n",
    "\n",
    "    print(\"Iniciando comparación de rendimiento...\")\n",
    "\n",
    "    # Definir implementaciones a comparar\n",
    "    implementaciones = {\n",
    "        'Apriori Original': apriori_original,\n",
    "        'Apriori Optimizado': apriori_con_poda_optimizada\n",
    "    }\n",
    "\n",
    "    # Generar datasets sintéticos\n",
    "    print(\"Generando datasets sintéticos...\")\n",
    "    datasets = {\n",
    "        'Dataset Pequeño': generar_dataset_sintetico(100, 20, 5),\n",
    "        'Dataset Mediano': generar_dataset_sintetico(1000, 50, 8)\n",
    "    }\n",
    "\n",
    "    # Definir umbrales de soporte a probar\n",
    "    min_supports = [0.1, 0.05, 0.02]\n",
    "\n",
    "    # Ejecutar comparación\n",
    "    print(\"Ejecutando comparación...\")\n",
    "    resultados = comparar_rendimiento(datasets, min_supports, implementaciones, parametros=None)\n",
    "\n",
    "    # Verificar si hay resultados\n",
    "    if not resultados:\n",
    "        print(\"La comparación no generó resultados.\")\n",
    "        return None\n",
    "\n",
    "    # Imprimir resultados en bruto para depuración\n",
    "    print(\"\\nResultados en bruto:\")\n",
    "    for impl_name, results in resultados.items():\n",
    "        print(f\"\\n{impl_name}:\")\n",
    "        for r in results:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "    # Visualizar resultados\n",
    "    print(\"\\nVisualizando resultados...\")\n",
    "    visualizar_resultados_simple(resultados, datasets, min_supports)\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Ejecutar la comparación\n",
    "print(\"Ejecutando la comparación de rendimiento...\")\n",
    "resultados = ejecutar_comparacion()\n",
    "print(\"Comparación completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicacion Fuerza bruta del algoritmo apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos una funcion de fuerza bruta del algoritmo para compara con apriori\n",
    "def apriori_fuerza_bruta(transactions, min_support=0.5, max_length=None):\n",
    "\n",
    "    # Convertimos transacciones a conjuntos si no lo son...\n",
    "    transactions = [set(transaction) for transaction in transactions]\n",
    "    n_transactions = len(transactions)\n",
    "    min_support_count = min_support * n_transactions\n",
    "\n",
    "    # encontrar todos los items únicos\n",
    "    all_items = set()\n",
    "    for transaction in transactions:\n",
    "        all_items.update(transaction)\n",
    "\n",
    "    print(f\"Número total de items únicos: {len(all_items)}\")\n",
    "\n",
    "    # Almacenamos\n",
    "    all_frequent_itemsets = {}\n",
    "\n",
    "    # observamos la longitud máxima si no se especifica (esto revisar info al respecto)\n",
    "    if max_length is None:\n",
    "        max_length = len(all_items)\n",
    "\n",
    "    # generar y evaluar todos los posibles itemsets de tamaño 1 hasta max_length\n",
    "    for k in range(1, max_length + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generar todos los posibles k-itemsets (sin poda)\n",
    "        all_candidates = list(combinations(all_items, k))\n",
    "        print(f\"Generados {len(all_candidates)} candidatos de tamaño {k}\")\n",
    "\n",
    "        # Conteo de soporte por candidato\n",
    "        itemset_counts = defaultdict(int)\n",
    "        for candidate in all_candidates:\n",
    "            candidate_set = frozenset(candidate)\n",
    "            for transaction in transactions:\n",
    "                if candidate_set.issubset(transaction):\n",
    "                    itemset_counts[candidate_set] += 1\n",
    "\n",
    "        # filtrico\n",
    "        k_frequent_itemsets = {itemset: count for itemset, count in itemset_counts.items()\n",
    "                              if count >= min_support_count}\n",
    "\n",
    "\n",
    "        all_frequent_itemsets.update(k_frequent_itemsets)\n",
    "\n",
    "        # estadísticas de esta iteración\n",
    "        end_time = time.time()\n",
    "        print(f\"Iteración {k}: Encontrados {len(k_frequent_itemsets)} itemsets frecuentes en {end_time - start_time:.4f} segundos\")\n",
    "\n",
    "        # Si no hay no habra en la siguiente\n",
    "        if not k_frequent_itemsets:\n",
    "            break\n",
    "\n",
    "    return all_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_lotes(transactions_list, min_support, batch_size=50000):\n",
    "\n",
    "    n_total = len(transactions_list)\n",
    "    print(f\"Procesando {n_total} transacciones con soporte mínimo {min_support}\")\n",
    "\n",
    "    # Fase 1: Contar 1-itemsets en lotes\n",
    "    item_counts = {}\n",
    "    n_batches = (n_total + batch_size - 1) // batch_size\n",
    "    print(f\"Fase 1: Generando 1-itemsets frecuentes en {n_batches} lotes.\")\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_total)\n",
    "        print(f\"  Procesando lote {i+1}/{n_batches} (transacciones {start_idx} - {end_idx})\")\n",
    "        batch_transactions = transactions_list[start_idx:end_idx]\n",
    "        for transaction in batch_transactions:\n",
    "            for item in transaction:\n",
    "                item_counts[item] = item_counts.get(item, 0) + 1\n",
    "\n",
    "    frequent_1_itemsets = []\n",
    "    support_dict = {}\n",
    "    for item, count in item_counts.items():\n",
    "        support = count / n_total\n",
    "        if support >= min_support:\n",
    "            frequent_1_itemsets.append([item])\n",
    "            support_dict[tuple([item])] = support\n",
    "    print(f\"Se encontraron {len(frequent_1_itemsets)} 1-itemsets frecuentes.\")\n",
    "\n",
    "    all_frequent_itemsets = {1: frequent_1_itemsets}\n",
    "    k = 1\n",
    "    while all_frequent_itemsets.get(k, []):\n",
    "        print(f\"Fase {k+1}: Generando {k+1}-itemsets frecuentes.\")\n",
    "        if len(all_frequent_itemsets[k]) > 1000:\n",
    "            print(f\"  Demasiados itemsets frecuentes ({len(all_frequent_itemsets[k])}). Limitando a 1000 con mayor soporte.\")\n",
    "            sorted_itemsets = sorted(all_frequent_itemsets[k],\n",
    "                                     key=lambda x: support_dict.get(tuple(x), 0),\n",
    "                                     reverse=True)[:1000]\n",
    "            all_frequent_itemsets[k] = sorted_itemsets\n",
    "\n",
    "        candidatos = generar_candidatos(all_frequent_itemsets[k], k, support_dict=support_dict)\n",
    "        if len(candidatos) > 100000:\n",
    "            print(f\"  Demasiados candidatos ({len(candidatos)}). Aumentando temporalmente el umbral de soporte.\")\n",
    "            temp_min_support = min_support * 2\n",
    "            filtered_itemsets = [itemset for itemset in all_frequent_itemsets[k] if support_dict.get(tuple(itemset), 0) >= temp_min_support]\n",
    "            candidatos = generar_candidatos(filtered_itemsets, k, support_dict=support_dict)\n",
    "\n",
    "        candidatos_podados = poda_apriori(candidatos, all_frequent_itemsets[k], k)\n",
    "        print(f\"  Generados {len(candidatos)} candidatos, {len(candidatos_podados)} después de poda.\")\n",
    "        if not candidatos_podados:\n",
    "            break\n",
    "\n",
    "        # Contar soporte de candidatos en lotes\n",
    "        candidato_counts = {tuple(c): 0 for c in candidatos_podados}\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_total)\n",
    "            print(f\"  Contando candidatos en lote {i+1}/{n_batches} (transacciones {start_idx} - {end_idx})\")\n",
    "            batch_transactions = transactions_list[start_idx:end_idx]\n",
    "            for transaction in batch_transactions:\n",
    "                transaction_set = set(transaction)\n",
    "                for candidato in candidatos_podados:\n",
    "                    if all(item in transaction_set for item in candidato):\n",
    "                        candidato_counts[tuple(candidato)] += 1\n",
    "\n",
    "        frequent_itemsets_k_plus_1 = []\n",
    "        for candidato, count in candidato_counts.items():\n",
    "            support = count / n_total\n",
    "            if support >= min_support:\n",
    "                frequent_itemsets_k_plus_1.append(list(candidato))\n",
    "                support_dict[tuple(sorted(candidato))] = support\n",
    "\n",
    "        print(f\"  Se encontraron {len(frequent_itemsets_k_plus_1)} {k+1}-itemsets frecuentes.\")\n",
    "        if frequent_itemsets_k_plus_1:\n",
    "            all_frequent_itemsets[k+1] = frequent_itemsets_k_plus_1\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "    return all_frequent_itemsets, support_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_reglas(itemsets_frecuentes, support_dict, min_confidence=0.5):\n",
    "\n",
    "    rules = []\n",
    "    for k in range(2, len(itemsets_frecuentes) + 1):\n",
    "        if k not in itemsets_frecuentes:\n",
    "            continue\n",
    "        for itemset in itemsets_frecuentes[k]:\n",
    "            itemset_support = support_dict.get(tuple(sorted(itemset)), 0)\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedente_items in combinations(itemset, i):\n",
    "                    antecedente = list(antecedente_items)\n",
    "                    consecuente = [item for item in itemset if item not in antecedente]\n",
    "                    if not antecedente or not consecuente:\n",
    "                        continue\n",
    "                    antecedente_support = support_dict.get(tuple(sorted(antecedente)), 0)\n",
    "                    if antecedente_support == 0:\n",
    "                        continue\n",
    "                    confidence = itemset_support / antecedente_support\n",
    "                    if confidence >= min_confidence:\n",
    "                        consecuente_support = support_dict.get(tuple(sorted(consecuente)), 0)\n",
    "                        if consecuente_support == 0:\n",
    "                            continue\n",
    "                        lift = confidence / consecuente_support\n",
    "                        rules.append({\n",
    "                            'antecedent': antecedente,\n",
    "                            'consequent': consecuente,\n",
    "                            'support': itemset_support,\n",
    "                            'confidence': confidence,\n",
    "                            'lift': lift\n",
    "                        })\n",
    "    if rules:\n",
    "        rulesDf = pd.DataFrame(rules)\n",
    "        rulesDf = rulesDf.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        return rulesDf\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['antecedent', 'consequent', 'support', 'confidence', 'lift'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecutar el algoritmo Apriori manual\n",
    "min_support = 0.01  # Ajusta según tu dataset\n",
    "start_time = time.time()\n",
    "\n",
    "# Aquí usamos la lista de transacciones obtenida en la SECCIÓN 1\n",
    "transactions_list = transactions  # Asegúrate de haber generado `transactions` previamente\n",
    "all_frequent_itemsets, support_dict = apriori_lotes(transactions_list, min_support)\n",
    "\n",
    "manual_time = time.time() - start_time\n",
    "print(f\"\\nTiempo del Apriori manual: {manual_time:.2f} segundos\")\n",
    "\n",
    "# Generar reglas a partir de los itemsets frecuentes\n",
    "rules_manual = generar_reglas(all_frequent_itemsets, support_dict, min_confidence=0.3)\n",
    "print(\"\\nReglas generadas manualmente (primeras 5):\")\n",
    "print(rules_manual.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentacion de la complejidad computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentar_complejidad_experimental():\n",
    "\n",
    "    print(\"=== ANÁLISIS DE COMPLEJIDAD COMPUTACIONAL DEL ALGORITMO APRIORI ===\")\n",
    "    print(\"Este análisis experimental valida las afirmaciones teóricas presentadas en el documento.\")\n",
    "\n",
    "    # 1. Generación de 1-itemsets\n",
    "    print(\"\\n1. Generación de 1-itemsets:\")\n",
    "    print(\"   Complejidad Teórica: O(n × m) donde n = número de transacciones, m = número de items únicos\")\n",
    "\n",
    "    print(\"\\n   Mediciones experimentales (tiempo en segundos):\")\n",
    "    print(\"   | Transacciones (n) | Items (m) | Tiempo | Ratio n×m/tiempo |\")\n",
    "    print(\"   |------------------|-----------|--------|------------------|\")\n",
    "\n",
    "    # Variar n manteniendo m constante\n",
    "    m_constante = 50\n",
    "    for n in [100, 500, 1000, 5000]:\n",
    "        dataset = generar_dataset_sintetico(n, m_constante, m_constante//4)\n",
    "\n",
    "        start_time = time.time()\n",
    "        item_counts = contar_items_individuales(dataset)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "        ratio = (n * m_constante) / (tiempo * 1000)  # Normalizado para mejor visualización\n",
    "\n",
    "        print(f\"   | {n:16d} | {m_constante:9d} | {tiempo:.6f} | {ratio:.2f} |\")\n",
    "\n",
    "    # Variar m manteniendo n constante\n",
    "    n_constante = 1000\n",
    "    for m in [20, 50, 100, 200]:\n",
    "        dataset = generar_dataset_sintetico(n_constante, m, m//4)\n",
    "\n",
    "        start_time = time.time()\n",
    "        item_counts = contar_items_individuales(dataset)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "        ratio = (n_constante * m) / (tiempo * 1000)  # Normalizado\n",
    "\n",
    "        print(f\"   | {n_constante:16d} | {m:9d} | {tiempo:.6f} | {ratio:.2f} |\")\n",
    "\n",
    "    # 2. Generación de Candidatos\n",
    "    print(\"\\n2. Generación de Candidatos (k+1 a partir de k-itemsets):\")\n",
    "    print(\"   Complejidad Teórica: O(l² × k) donde l = número de k-itemsets frecuentes, k = tamaño del itemset\")\n",
    "\n",
    "    print(\"\\n   Mediciones experimentales (tiempo en segundos):\")\n",
    "    print(\"   | Itemsets (l) | Tamaño (k) | Tiempo | Ratio l²×k/tiempo |\")\n",
    "    print(\"   |-------------|------------|--------|-------------------|\")\n",
    "\n",
    "    # Variar l manteniendo k constante\n",
    "    k_constante = 3\n",
    "    for l in [10, 50, 100, 200]:\n",
    "        # Crear itemsets frecuentes sintéticos\n",
    "        itemsets = [frozenset([f'item_{i}' for i in range(j, j+k_constante)])\n",
    "                   for j in range(l)]\n",
    "\n",
    "        start_time = time.time()\n",
    "        candidatos = generar_candidatos(itemsets, k_constante)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "        ratio = (l**2 * k_constante) / (tiempo * 1000)  # Normalizado\n",
    "\n",
    "        print(f\"   | {l:11d} | {k_constante:10d} | {tiempo:.6f} | {ratio:.2f} |\")\n",
    "\n",
    "    # Variar k manteniendo l constante\n",
    "    l_constante = 100\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        # Crear itemsets frecuentes sintéticos\n",
    "        itemsets = [frozenset([f'item_{i}' for i in range(j, j+k)])\n",
    "                   for j in range(l_constante)]\n",
    "\n",
    "        start_time = time.time()\n",
    "        candidatos = generar_candidatos(itemsets, k)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "        ratio = (l_constante**2 * k) / (tiempo * 1000)  # Normalizado\n",
    "\n",
    "        print(f\"   | {l_constante:11d} | {k:10d} | {tiempo:.6f} | {ratio:.2f} |\")\n",
    "\n",
    "    # 3. Poda Apriori\n",
    "    print(\"\\n3. Poda Apriori:\")\n",
    "    print(\"   Complejidad Teórica sin optimización: O(c × k × l)\")\n",
    "    print(\"   Complejidad Teórica con optimización: O(c × k)\")\n",
    "\n",
    "    print(\"\\n   Mediciones experimentales (tiempo en segundos):\")\n",
    "    print(\"   | Candidatos (c) | Tamaño (k) | Itemsets (l) | Sin optimización | Con optimización | Mejora (%) |\")\n",
    "    print(\"   |---------------|------------|--------------|------------------|------------------|------------|\")\n",
    "\n",
    "    for c in [20, 50, 100]:\n",
    "        for k in [3, 4]:\n",
    "            l = c // 2  # Relación arbitraria entre c y l para el experimento\n",
    "\n",
    "            # Crear datos sintéticos\n",
    "            candidatos = [sorted([f'item_{i}' for i in range(j, j+k)])\n",
    "                         for j in range(c)]\n",
    "            itemsets_frecuentes = [tuple(sorted([f'item_{i}' for i in range(j, j+k-1)]))\n",
    "                                  for j in range(l)]\n",
    "\n",
    "            # Medir tiempo sin optimización\n",
    "            start_time = time.time()\n",
    "            poda_apriori(candidatos, itemsets_frecuentes, k)\n",
    "            end_time = time.time()\n",
    "            tiempo_sin_opt = end_time - start_time\n",
    "\n",
    "            # Medir tiempo con optimización\n",
    "            start_time = time.time()\n",
    "            apriori_con_poda_optimizada(candidatos, itemsets_frecuentes, k)\n",
    "            end_time = time.time()\n",
    "            tiempo_con_opt = end_time - start_time\n",
    "\n",
    "            # Calcular mejora\n",
    "            if tiempo_sin_opt > 0:\n",
    "                mejora = (1 - tiempo_con_opt / tiempo_sin_opt) * 100\n",
    "            else:\n",
    "                mejora = 0\n",
    "\n",
    "            print(f\"   | {c:13d} | {k:10d} | {l:12d} | {tiempo_sin_opt:.6f} | {tiempo_con_opt:.6f} | {mejora:.2f} |\")\n",
    "\n",
    "    # 4. Cálculo de Soporte\n",
    "    print(\"\\n4. Cálculo de Soporte para Candidatos:\")\n",
    "    print(\"   Complejidad Teórica: O(n × c × k)\")\n",
    "\n",
    "    print(\"\\n   Mediciones experimentales (tiempo en segundos):\")\n",
    "    print(\"   | Transacciones (n) | Candidatos (c) | Tamaño (k) | Tiempo | Ratio n×c×k/tiempo |\")\n",
    "    print(\"   |-------------------|---------------|------------|--------|--------------------|\")\n",
    "\n",
    "    for n in [100, 500, 1000]:\n",
    "        for c in [10, 50]:\n",
    "            for k in [2, 3, 4]:\n",
    "                # Generar datos sintéticos\n",
    "                dataset = generar_dataset_sintetico(n, 100, 20)\n",
    "                candidatos = [frozenset([f'item_{i}' for i in range(j, j+k)])\n",
    "                             for j in range(c)]\n",
    "\n",
    "                # Medir tiempo\n",
    "                start_time = time.time()\n",
    "                count_support_optimized(dataset, candidatos)\n",
    "                end_time = time.time()\n",
    "\n",
    "                tiempo = end_time - start_time\n",
    "                ratio = (n * c * k) / (tiempo * 1000)  # Normalizado\n",
    "\n",
    "                print(f\"   | {n:17d} | {c:13d} | {k:10d} | {tiempo:.6f} | {ratio:.2f} |\")\n",
    "\n",
    "    # 5. Complejidad Total y Factores que Afectan el Rendimiento\n",
    "    print(\"\\n=== COMPLEJIDAD TOTAL Y FACTORES QUE AFECTAN EL RENDIMIENTO ===\")\n",
    "\n",
    "    # 5.1 Efecto del umbral de soporte mínimo\n",
    "    print(\"\\n5.1 Efecto del Umbral de Soporte Mínimo:\")\n",
    "    print(\"   | Soporte Mínimo | Tiempo (s) | Itemsets Encontrados |\")\n",
    "    print(\"   |----------------|------------|----------------------|\")\n",
    "\n",
    "    dataset_fijo = generar_dataset_sintetico(1000, 50, 15)\n",
    "    for min_support in [0.1, 0.05, 0.02, 0.01]:\n",
    "        start_time = time.time()\n",
    "        frequent_itemsets = apriori_original(dataset_fijo, min_support=min_support)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "\n",
    "        print(f\"   | {min_support:.4f} | {tiempo:.6f} | {len(frequent_itemsets):20d} |\")\n",
    "\n",
    "    # 5.2 Efecto del número de items únicos\n",
    "    print(\"\\n5.2 Efecto del Número de Items Únicos:\")\n",
    "    print(\"   | Items Únicos | Tiempo (s) | Itemsets Encontrados |\")\n",
    "    print(\"   |-------------|------------|----------------------|\")\n",
    "\n",
    "    n_fijo = 1000\n",
    "    min_support_fijo = 0.05\n",
    "    for m in [20, 50, 100, 200]:\n",
    "        dataset = generar_dataset_sintetico(n_fijo, m, m//3)\n",
    "\n",
    "        start_time = time.time()\n",
    "        frequent_itemsets = apriori_original(dataset, min_support=min_support_fijo)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "\n",
    "        print(f\"   | {m:11d} | {tiempo:.6f} | {len(frequent_itemsets):20d} |\")\n",
    "\n",
    "    # 5.3 Efecto de la longitud de las transacciones\n",
    "    print(\"\\n5.3 Efecto de la Longitud de las Transacciones:\")\n",
    "    print(\"   | Longitud Media | Tiempo (s) | Itemsets Encontrados |\")\n",
    "    print(\"   |---------------|------------|----------------------|\")\n",
    "\n",
    "    n_fijo = 1000\n",
    "    m_fijo = 100\n",
    "    min_support_fijo = 0.05\n",
    "    for longitud in [10, 20, 30, 40]:\n",
    "        dataset = generar_dataset_sintetico(n_fijo, m_fijo, longitud)\n",
    "\n",
    "        start_time = time.time()\n",
    "        frequent_itemsets = apriori_original(dataset, min_support=min_support_fijo)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tiempo = end_time - start_time\n",
    "\n",
    "        print(f\"   | {longitud:13d} | {tiempo:.6f} | {len(frequent_itemsets):20d} |\")\n",
    "\n",
    "    # Visualización gráfica de los resultados\n",
    "    print(\"\\n=== VISUALIZACIÓN GRÁFICA DE LOS RESULTADOS ===\")\n",
    "    print(\"Generando gráficos para visualizar los resultados experimentales...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_resultados_experimentales():\n",
    "    \"\"\"Genera visualizaciones gráficas de los resultados experimentales\"\"\"\n",
    "    # Datos de ejemplo para las gráficas (estos deberían ser reemplazados por datos reales)\n",
    "    # 1. Efecto del soporte mínimo\n",
    "    soportes = [0.1, 0.05, 0.02, 0.01]\n",
    "    tiempos = [0.5, 1.2, 3.5, 8.7]\n",
    "    itemsets = [10, 25, 80, 210]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Gráfico 1: Tiempo vs Soporte mínimo\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(soportes, tiempos, 'o-', color='blue')\n",
    "    plt.title('Tiempo de ejecución vs Soporte mínimo')\n",
    "    plt.xlabel('Soporte mínimo')\n",
    "    plt.ylabel('Tiempo (s)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Gráfico 2: Itemsets encontrados vs Soporte mínimo\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(soportes, itemsets, 'o-', color='green')\n",
    "    plt.title('Itemsets frecuentes vs Soporte mínimo')\n",
    "    plt.xlabel('Soporte mínimo')\n",
    "    plt.ylabel('Número de itemsets')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Gráfico 3: Complejidad de generación de candidatos\n",
    "    l_values = [10, 50, 100, 200]\n",
    "    tiempos_gen = [0.001, 0.02, 0.08, 0.3]\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(l_values, tiempos_gen, 'o-', color='red')\n",
    "    plt.title('Tiempo de generación de candidatos vs Tamaño de L')\n",
    "    plt.xlabel('Número de itemsets frecuentes (l)')\n",
    "    plt.ylabel('Tiempo (s)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Gráfico 4: Comparación de poda con y sin optimización\n",
    "    candidatos = [20, 50, 100]\n",
    "    tiempos_sin_opt = [0.01, 0.05, 0.2]\n",
    "    tiempos_con_opt = [0.002, 0.01, 0.04]\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    x = np.arange(len(candidatos))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, tiempos_sin_opt, width, label='Sin optimización')\n",
    "    plt.bar(x + width/2, tiempos_con_opt, width, label='Con optimización')\n",
    "\n",
    "    plt.title('Comparación de métodos de poda')\n",
    "    plt.xlabel('Número de candidatos')\n",
    "    plt.xticks(x, candidatos)\n",
    "    plt.ylabel('Tiempo (s)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'c:\\Users\\stick\\Desktop\\4Geeks Academy Projects\\Apriori-Latex-Project-Final\\Apriori-Latex-Project-Final\\complejidad_experimental.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    documentar_complejidad_experimental()\n",
    "    visualizar_resultados_experimentales()\n",
    "except Exception as e:\n",
    "    print(f\"Error al ejecutar la documentación experimental: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
